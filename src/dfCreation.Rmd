---
title: "561 Final Project: Dataframe Creation"
author: "Team 2"
date: "11/23/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(e1071) 
library(keras)
library(tensorflow)
library(entropy)

#Use this in command line if tensorflow.keras not found error
#use_condaenv("r-tensorflow")
```

This script creates the final dataframe that we use as inputs to our models, using the following steps:
- Generate the initial dataframe and populate it with simple summary statistic features
- Add in contour features
- Add in pooling features
- Save the unstandardized data to CSVs
- Create the dataframe of standardized features
- Save the standardized data to CSVs

Reading in data:

```{r}
# Set working directory as needed
setwd("/Users/margaretsabelhaus/Documents/GitHub/csci561-finalproject-team2")

load("./data/High/high_train.RData")
load("./data/Medium/med_train.RData")
load("./data/Low/low_train.RData")
load("./data/High/high_validate.RData")
load("./data/Medium/med_validate.RData")
load("./data/Low/low_validate.RData")
load("./data/test/test.RData")
```

```{r}
# Set working directory as needed, then load in data
#load("./data/test/test.RData")
```

We first use Dr. Hammerling's code to initialize our dataframe of [datasets, features]:

```{r}
df = data.frame()
ds_list = list(high_train, med_train, low_train, high_validate, med_validate, low_validate, test_df)
classifications = c("high", "med", "low")
class_index = rep(c(1, 2, 3), 2)
for (i in 1:length(ds_list)){
  l = length(ds_list[[i]]$mat)
  sum_feature = rep(NA, l)
  for(j in 1:l){
    sum_feature[j] = sum(ds_list[[i]]$mat[[j]])
  }
  classification = classifications[class_index[i]]
  
  # -- Margaret adding in; this is dummy for whether or not the 
  # dataset is in our validation set.
  test = ifelse(i %in% c(4,5,6),1,0)
  
  df = rbind(df, data.frame("sum"=sum_feature, "classification"=rep(classification, l), "validate"=rep(test,l)))
}
df$classification = as.factor(df$classification)

# Append a new feature, this one is the number of zeroes in each dataset, a measure of the sparsity of the dataset.
ds_list = list(high_train, med_train, low_train, high_validate, med_validate, low_validate, test_df)
z = c()
for (i in 1:length(ds_list)){
  l = length(ds_list[[i]]$mat)
  zero_feature = rep(NA, l)
  for(j in 1:l){
    zero_feature[j] = sum(ds_list[[i]]$mat[[j]] == 0)
  }
  z = c(z, zero_feature)
}
df = cbind(df, data.frame("zeroes"=z))
```

# Creating Summary Statistic Features

This loop populates separate lists with values for a variety of summary statistic features.

```{r}
# Initialize empty vectors to populate with values for each dataset
norm = c()
min = c()
max = c()
q3 = c()
q1 = c()
iqr = c()
mean = c()
med = c()
std = c()
range = c()
skew = c()
kurtosis = c()
aboveMean = c()
entropy = c()
#magOrder = c()

# Loop through datasets and populate vectors
for (i in 1:length(ds_list)){
  numMats = length(ds_list[[i]]$mat)
  
  for (j in 1:numMats){
  
    mat = ds_list[[i]]$mat[[j]]
    
    # Norm
    norm = c(norm, norm(mat))
    
    # Min/Max
    min = c(min, min(mat))
    max = c(max, max(mat))
    
    # Quantiles, IQR
    q1 = c(q1, quantile(mat, .25))
    q3 = c(q3, quantile(mat, .75))
    iqr = c(iqr, quantile(mat, .25) - quantile(mat, .75))
    
    # Measures of center
    med = c(med, median(mat))
    mean = c(mean, mean(mat))
    
    # Measures of spread
    std = c(std, sd(mat))
    range = c(range, max(mat) - min(mat))
    
    # Skewness and Kurtosis
    skew = c(skew, skewness(mat))
    kurtosis = c(kurtosis, kurtosis(mat))
    
    # How many elements > mean
    aboveMean = c(aboveMean, length(which(mat > mean(mat))))
    
    # Shannon entropy
    entropy = c(entropy, entropy(mat))
    
    # log10 (max value) - log10 (min value)
    #magOrder = c(magOrder, log10(max(mat))-log10(min(mat)))
  }
}
```

We then simply cbind these lists onto the dataframe and name the features appropriately.

```{r}
# Attaching all of our newly-created features...
df = cbind(df, data.frame("norm"=norm))
df = cbind(df, data.frame("min"=min))
df = cbind(df, data.frame("max"=max))
df = cbind(df, data.frame("q1"=q1))
df = cbind(df, data.frame("q3"=q3))
df = cbind(df, data.frame("iqr"=iqr))
df = cbind(df, data.frame("mean"=mean))
df = cbind(df, data.frame("med"=med))
df = cbind(df, data.frame("std"=std))
df = cbind(df, data.frame("range"=range))
df = cbind(df, data.frame("skew"=skew))
df = cbind(df, data.frame("kurtosis"=kurtosis))
df = cbind(df, data.frame("aboveMeanCount"=aboveMean))
df = cbind(df, data.frame("entropy"=entropy))
#df = cbind(df, data.frame("magOrder"=magOrder))
```

Below we take the log of certain variables.

```{r}
# Zeroes
zeroes_fix = df$zeroes
zeroes_fix = ifelse(zeroes_fix==0,zeroes_fix+.1,zeroes_fix)
zeros.log = log(zeroes_fix)
df = cbind(df, data.frame("zeros.log"=zeros.log))
#hist(zeros.log)
#skewness(df$zeros.log)

# Norm
norm_fix = df$norm
norm_fix = ifelse(norm_fix==0,norm_fix+.1,norm_fix)
norm.log = log(norm_fix)
df = cbind(df, data.frame("norm.log"=norm.log))
#hist(norm.log)
#print(skewness(norm.log))

# Min
min_fix = df$min
min_fix = ifelse(min_fix==0,min_fix+.1,min_fix)
min.log = log(min_fix)
df = cbind(df, data.frame("min.log"=min.log))
#hist(min.log)
#print(skewness(min.log))

# Max
max_fix = df$max
max_fix = ifelse(max_fix==0,max_fix+.1,max_fix)
max.log = log(max_fix)
df = cbind(df, data.frame("max.log"=max.log))
#hist(max.log)
#print(skewness(max.log))

# Q1
q1_fix = df$q1
q1_fix = ifelse(q1_fix==0,q1_fix+.1,q1_fix)
q1.log = log(q1_fix)
df = cbind(df, data.frame("q1.log"=q1.log))
#hist(q1.log)
#print(skewness(q1.log))

# Q3
q3_fix = df$q3
q3_fix = ifelse(q3_fix==0,q3_fix+.1,q3_fix)
q3.log = log(q3_fix)
df = cbind(df, data.frame("q3.log"=q3.log))
#hist(q3.log)
#print(skewness(q3.log))

# Med
med_fix = df$med
med_fix = ifelse(med_fix==0,med_fix+.1,med_fix)
med.log = log(med_fix)
df = cbind(df, data.frame("med.log"=med.log))
#hist(med.log)
#print(skewness(med.log))

# Std
std_fix = df$std
std_fix = ifelse(std_fix==0,std_fix+.1,std_fix)
std.log = log(std_fix)
df = cbind(df, data.frame("std.log"=std.log))
#hist(std.log)
#print(skewness(std.log))

# Range
range_fix = df$range
range_fix = ifelse(range_fix==0,range_fix+.1,range_fix)
range.log = log(range_fix)
df = cbind(df, data.frame("range.log"=range.log))
#hist(range.log)
#print(skewness(range.log))

# Skew
skew_fix = df$skew
skew_fix = ifelse(skew_fix==0,skew_fix+.1,skew_fix)
skew.log = log(skew_fix)
df = cbind(df, data.frame("skew.log"=skew.log))
#hist(skew.log)
#print(skewness(skew.log))

# Kurtosis
kurtosis_fix = df$kurtosis
kurtosis_fix = ifelse(kurtosis_fix==0,kurtosis_fix+.1,kurtosis_fix)
kurtosis.log = log(kurtosis_fix)
df = cbind(df, data.frame("kurtosis.log"=kurtosis.log))
#hist(kurtosis.log)
#print(skewness(kurtosis.log))

# AboveMeanCount
aboveMeanCount_fix = df$aboveMeanCount
aboveMeanCount_fix = ifelse(aboveMeanCount_fix==0,aboveMeanCount_fix+.1,aboveMeanCount_fix)
aboveMeanCount.log = log(aboveMeanCount_fix)
df = cbind(df, data.frame("aboveMeanCount.log"=aboveMeanCount.log))
#hist(aboveMeanCount.log)
#print(skewness(aboveMeanCount.log))
```

We then add in values for Jose's contour variables. This data is saved to CSVs in our /data folder (one CSV for traing/validation, one for test.) We do a simple cbind() to attach the features.

```{r}
contour = read_csv("../data/contourData.csv")
contourtest = read_csv("../data/contourDataTest.csv")
contour = rbind(contour,contourtest)
```

```{r}
df = cbind(df,contour)
```

We next add in values for Collin's pooling variables. This data is saved to CSVs in our /data folder (one CSV for traing/validation, one for test.) We do a simple cbind() to attach the features.

```{r}
pooling = read_csv("pooling/train_validation_poolingFeatures.csv")
poolingtest = read_csv("pooling/testing_poolingFeatures.csv")
```

We need to clean up these CSVs a bit before adding to 'df'.

```{r}
pooling = pooling[-c(1:4),-1]
poolingtest = poolingtest[-c(1:4),-1]
pooling = rbind(pooling, poolingtest)
```

Add to 'df'
```{r}
df = cbind(df,pooling)
```

We then split 'df' into train/validation and test sets, and save those separately to CSVs.

```{r}
df_test = df[which(is.na(df$classification)),]
df =  df[-which(is.na(df$classification)),]
```

```{r}
# Change file path as needed
#write.csv(df, "/Users/margaretsabelhaus/Documents/GitHub/csci561-finalproject-team2/data/df.csv", row.names = FALSE)
#write.csv(df_test, "/Users/margaretsabelhaus/Documents/GitHub/csci561-finalproject-team2/data/df_test.csv", row.names = FALSE)
```

Then, we recombine the data so we can easily create the standardized dataframe.

```{r}
df = rbind(df,df_test)
```

Now we standardize our features:

```{r}
# Don't scale the target or dummy for whether it's in train/test set
standardized.df = data.frame(scale(df[,c("sum","norm","min","max","q1","q3",
                                         "iqr","mean","med","std","range","skew",
                                         "kurtosis")]))
standardized.df$classification = df$classification
standardized.df$validate = df$validate
standardized.df$zeroes = df$zeroes
standardized.df$aboveMeanCount = df$aboveMeanCount
standardized.df$entropy = df$entropy
standardized.df$zeros.log = df$zeros.log
standardized.df$norm.log = df$norm.log
standardized.df$min.log = df$min.log
standardized.df$max.log = df$max.log
standardized.df$q1.log = df$q1.log
standardized.df$q3.log = df$q3.log
standardized.df$med.log = df$med.log
standardized.df$std.log = df$std.log
standardized.df$range.log = df$range.log
standardized.df$skew.log = df$skew.log
standardized.df$kurtosis.log = df$kurtosis.log
standardized.df$aboveMeanCount.log = df$aboveMeanCount.log
standardized.df$contourLength = df$contourLength
standardized.df$contourCount = df$contourCount
standardized.df$contourCountLSD = df$contourCountLSD
standardized.df$contourAvgLength = df$contourAvgLength
standardized.df$contourLengthLSD = df$contourLengthLSD
standardized.df$contourAvgLengthLSD = df$contourAvgLengthLSD
standardized.df$VarPool_75x25_MaxStat = df$VarPool_75x25_MaxStat
standardized.df$StdPool_75x25_VarStat = df$StdPool_75x25_VarStat
standardized.df$StdPool_75x25_StdStat = df$StdPool_75x25_StdStat
standardized.df$VarPool_75x25_StdStat = df$VarPool_75x25_StdStat
standardized.df$VarPool_75x25_MeanStat = df$VarPool_75x25_MeanStat
```

We then split 'standardizeddf' into train/validation and test sets, and save those separately to CSVs.

```{r}
sdf_test = standardized.df[which(is.na(standardized.df$classification)),]
standardized.df =  standardized.df[-which(is.na(standardized.df$classification)),]
```

```{r}
# Change file path as needed
#write.csv(standardized.df, "/Users/margaretsabelhaus/Documents/GitHub/csci561-finalproject-team2/data/standardizeddf.csv", row.names = FALSE)
#write.csv(sdf_test, "/Users/margaretsabelhaus/Documents/GitHub/csci561-finalproject-team2/data/standardizeddf_test.csv", row.names = FALSE)
```

