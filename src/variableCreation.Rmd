---
title: "561 Final Project"
author: "Team 2"
date: "11/23/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(Matrix)
library(dplyr)
#library(resample)
#library(ISLR)
#library(tree)
#library(gbm)
#library(MASS)
#library(class)
library(e1071) 
library(keras)
library(tensorflow)

#Use this in command line if tensorflow.keras not found error
#use_condaenv("r-tensorflow")
```

Reading in data:

```{r}
# Set working directory as needed
#setwd("~/Documents/fall2021/DSCI561/finalProject")

load("./data/High/high_train.RData")
load("./data/Medium/med_train.RData")
load("./data/Low/low_train.RData")
load("./data/High/high_validate.RData")
load("./data/Medium/med_validate.RData")
load("./data/Low/low_validate.RData")
```

Try plotting:

```{r}
# Select data and axis labels
n=170
selected_dataset = med_train # copy of high_train list of datasets
l = length(selected_dataset$mat) # number of datasets in high_train
latitudes = round(as.numeric(colnames(selected_dataset$mat[[n]])))
longitudes = as.numeric(rownames(selected_dataset$mat[[n]]))
dataset = selected_dataset$mat[[n]] 
varname = selected_dataset$var[n]

# Plot
image(dataset, main=varname, col = hcl.colors(100, "Blue-Red"),
      axes=FALSE, xlab="Longitude", ylab="Latitude")
axis(3, at=seq(0,1, length=7), labels=longitudes[seq(1, 288, length.out=7)],
     lwd=0, pos=-0.2, outer=T)
axis(2, at=seq(1,0, length=9), labels=latitudes[seq(1, 192, length.out=9)],
     lwd=0, pos=0)
```

Let's use Dr. Hammerling's code to initialize our dataframe of [datasets, features]:

```{r}
df = data.frame()
ds_list = list(high_train, med_train, low_train, high_validate, med_validate, low_validate)
classifications = c("high", "med", "low")
class_index = rep(c(1, 2, 3), 2)
for (i in 1:length(ds_list)){
  l = length(ds_list[[i]]$mat)
  sum_feature = rep(NA, l)
  for(j in 1:l){
    sum_feature[j] = sum(ds_list[[i]]$mat[[j]])
  }
  classification = classifications[class_index[i]]
  
  # -- Margaret adding in; this is dummy for whether or not the 
  # dataset is in our validation set.
  test = ifelse(i %in% c(4,5,6),1,0)
  
  df = rbind(df, data.frame("sum"=sum_feature, "classification"=rep(classification, l), "validate"=rep(test,l)))
}
df$classification = as.factor(df$classification)

# Append a new feature, this one is the number of zeroes in each dataset, a measure of the sparsity of the dataset.
ds_list = list(high_train, med_train, low_train, high_validate, med_validate, low_validate)
z = c()
for (i in 1:length(ds_list)){
  l = length(ds_list[[i]]$mat)
  zero_feature = rep(NA, l)
  for(j in 1:l){
    zero_feature[j] = sum(ds_list[[i]]$mat[[j]] == 0)
  }
  z = c(z, zero_feature)
}
df = cbind(df, data.frame("zeroes"=z))
```

# Creating Summary Statistic Features

I'm going to loop through the datasets to create a bunch of new features. I'm sure there's a faster way to do this, but let's keep it simple for now 

```{r}
# Initialize empty vectors to populate with values for each dataset
norm = c()
min = c()
max = c()
q3 = c()
q1 = c()
iqr = c()
mean = c()
med = c()
std = c()
range = c()
skew = c()
kurtosis = c()
aboveMean = c()

# Loop through datasets and populate vectors
for (i in 1:length(ds_list)){
  numMats = length(ds_list[[i]]$mat)
  
  for (j in 1:numMats){
  
    mat = ds_list[[i]]$mat[[j]]
    
    # Norm
    norm = c(norm, norm(mat))
    
    # Min/Max
    min = c(min, min(mat))
    max = c(max, max(mat))
    
    # Quantiles, IQR
    q1 = c(q1, quantile(mat, .25))
    q3 = c(q3, quantile(mat, .75))
    iqr = c(iqr, quantile(mat, .25) - quantile(mat, .75))
    
    # Measures of center
    med = c(med, median(mat))
    mean = c(mean, mean(mat))
    
    # Measures of spread
    std = c(std, sd(mat))
    range = c(range, max(mat) - min(mat))
    
    # Skewness and Kurtosis
    skew = c(skew, skewness(mat))
    kurtosis = c(kurtosis, kurtosis(mat))
    
    # How many elements > mean
    aboveMean = c(aboveMean, length(which(mat > mean(mat))))
  }
}
```

```{r}
# Attaching all of our newly-created features...
df = cbind(df, data.frame("norm"=norm))
df = cbind(df, data.frame("min"=min))
df = cbind(df, data.frame("max"=max))
df = cbind(df, data.frame("q1"=q1))
df = cbind(df, data.frame("q3"=q3))
df = cbind(df, data.frame("iqr"=iqr))
df = cbind(df, data.frame("mean"=mean))
df = cbind(df, data.frame("med"=med))
df = cbind(df, data.frame("std"=std))
df = cbind(df, data.frame("range"=range))
df = cbind(df, data.frame("skew"=skew))
df = cbind(df, data.frame("kurtosis"=kurtosis))
df = cbind(df, data.frame("aboveMeanCount"=aboveMean))
```

'df' is our (current) finalized dataframe with all our new features.

Now we standardize our features:

```{r}
# Don't scale the target or dummy for whether it's in train/test set
standardized.df = data.frame(scale(df[,-c(2,3)]))
standardized.df$classification = df$classification
standardized.df$validate = df$validate
```

We can play around with which features, if any, we want to scale. All you need to do is exclude those you don't want scaled in line 174, then add them to the standardized.df as per lines 175 and 176.

# Applying convolutions

Convolutions and max pooling in R: https://rpubs.com/eR_ic/conv_pools.

Here are a few things we need to know first about the images we are dealing with:

  - All images have dimensions 288 x 192
  - All images  have 1 depth of 1 (binary).
  - We visualize this 1 dimensional range of values on a red-blue color scale.
  - The range of values for each image differs by the type of data
  - Number of images per dataset are as follows:
    - low_train: 110
    - med_train: 240
    - high_train: 360
    - low_validate: 40
    - med_validate: 90
    - high_validate: 140
  - Therefore there are a total of 980 images (270 validation + 710 training)

In order to perform convolutions we will need to. . .

  1. Reshape the data into single arrays for training, testing, and validation.

  2. Normalize the pixel values in each image by
   - Adjusting the range to have a 0 minimum
   - Divide by maximum value in each image

  3. Two paths: (https://stackoverflow.com/questions/43485361/whats-the-difference-between-conv-layer-and-pooling-layer-in-cnn/43491146)
   - Pooling
   - Convolution + Pooling


```{r}
#Get dataset lengths
lt = length(low_train$mat)
mt = length(med_train$mat)
ht = length(high_train$mat)
lv = length(low_validate$mat)
mv = length(med_validate$mat)
hv = length(high_validate$mat)

#Get pixel values
training_images = c(low_train$mat, med_train$mat, high_train$mat)
validation_images = c(low_validate$mat, med_validate$mat, high_validate$mat)

#Reshape data into single array
training_images_rs = array_reshape(training_images, c(lt+mt+ht, 288, 192, 1))
validation_images_rs = array_reshape(validation_images, c(lv+mv+hv, 288, 192, 1))
```

  2. Normalize the data using the max value in each of the images.

  ```{r}
  #Normalize the data (divide by the largest pixel value in each image).
  ```

  3. Instantiate the convolution.
  
```{r}
#Instantiate the convolution
model <- keras_model_sequential()
  # adding the first convolution layer with 64 3by3 filter
  # we add a color depth of 1 since convolutions operate over 3D tensors
  layer_conv_2d(input_shape = c(288, 192, 1), filters = 6, kernel_size = c(3,3), activation = 'relu')
  # adding a max pooling layer which halves the dimensions
  layer_max_pooling_2d(pool_size = c(2,2))
  # adding a second convolution layer which filters the results
  # from the previous layer
  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = 'relu') # adding a pooling layer
  layer_max_pooling_2d(pool_size = c(2,2))

```


