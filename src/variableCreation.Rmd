---
title: "561 Final Project"
author: "Team 2"
date: "11/23/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(Matrix)
library(dplyr)
#library(resample)
#library(ISLR)
#library(tree)
#library(gbm)
#library(MASS)
#library(class)
library(e1071) 
library(keras)
library(tensorflow)
library(entropy)

#Use this in command line if tensorflow.keras not found error
#use_condaenv("r-tensorflow")
```

Reading in data:

```{r}
# Set working directory as needed
setwd("/Users/margaretsabelhaus/Documents/GitHub/csci561-finalproject-team2")

load("./data/High/high_train.RData")
load("./data/Medium/med_train.RData")
load("./data/Low/low_train.RData")
load("./data/High/high_validate.RData")
load("./data/Medium/med_validate.RData")
load("./data/Low/low_validate.RData")
```

Try plotting:

```{r}
# Select data and axis labels
n=170
selected_dataset = med_train # copy of high_train list of datasets
l = length(selected_dataset$mat) # number of datasets in high_train
latitudes = round(as.numeric(colnames(selected_dataset$mat[[n]])))
longitudes = as.numeric(rownames(selected_dataset$mat[[n]]))
dataset = selected_dataset$mat[[n]] 
varname = selected_dataset$var[n]

# Plot
image(dataset, main=varname, col = hcl.colors(100, "Blue-Red"),
      axes=FALSE, xlab="Longitude", ylab="Latitude")
axis(3, at=seq(0,1, length=7), labels=longitudes[seq(1, 288, length.out=7)],
     lwd=0, pos=-0.2, outer=T)
axis(2, at=seq(1,0, length=9), labels=latitudes[seq(1, 192, length.out=9)],
     lwd=0, pos=0)
```

Let's use Dr. Hammerling's code to initialize our dataframe of [datasets, features]:

```{r}
df = data.frame()
ds_list = list(high_train, med_train, low_train, high_validate, med_validate, low_validate)
classifications = c("high", "med", "low")
class_index = rep(c(1, 2, 3), 2)
for (i in 1:length(ds_list)){
  l = length(ds_list[[i]]$mat)
  sum_feature = rep(NA, l)
  for(j in 1:l){
    sum_feature[j] = sum(ds_list[[i]]$mat[[j]])
  }
  classification = classifications[class_index[i]]
  
  # -- Margaret adding in; this is dummy for whether or not the 
  # dataset is in our validation set.
  test = ifelse(i %in% c(4,5,6),1,0)
  
  df = rbind(df, data.frame("sum"=sum_feature, "classification"=rep(classification, l), "validate"=rep(test,l)))
}
df$classification = as.factor(df$classification)

# Append a new feature, this one is the number of zeroes in each dataset, a measure of the sparsity of the dataset.
ds_list = list(high_train, med_train, low_train, high_validate, med_validate, low_validate)
z = c()
for (i in 1:length(ds_list)){
  l = length(ds_list[[i]]$mat)
  zero_feature = rep(NA, l)
  for(j in 1:l){
    zero_feature[j] = sum(ds_list[[i]]$mat[[j]] == 0)
  }
  z = c(z, zero_feature)
}
df = cbind(df, data.frame("zeroes"=z))
```

# Creating Summary Statistic Features

I'm going to loop through the datasets to create a bunch of new features. I'm sure there's a faster way to do this, but let's keep it simple for now 

```{r}
# Initialize empty vectors to populate with values for each dataset
norm = c()
min = c()
max = c()
q3 = c()
q1 = c()
iqr = c()
mean = c()
med = c()
std = c()
range = c()
skew = c()
kurtosis = c()
aboveMean = c()
entropy = c()
#magOrder = c()

# Loop through datasets and populate vectors
for (i in 1:length(ds_list)){
  numMats = length(ds_list[[i]]$mat)
  
  for (j in 1:numMats){
  
    mat = ds_list[[i]]$mat[[j]]
    
    # Norm
    norm = c(norm, norm(mat))
    
    # Min/Max
    min = c(min, min(mat))
    max = c(max, max(mat))
    
    # Quantiles, IQR
    q1 = c(q1, quantile(mat, .25))
    q3 = c(q3, quantile(mat, .75))
    iqr = c(iqr, quantile(mat, .25) - quantile(mat, .75))
    
    # Measures of center
    med = c(med, median(mat))
    mean = c(mean, mean(mat))
    
    # Measures of spread
    std = c(std, sd(mat))
    range = c(range, max(mat) - min(mat))
    
    # Skewness and Kurtosis
    skew = c(skew, skewness(mat))
    kurtosis = c(kurtosis, kurtosis(mat))
    
    # How many elements > mean
    aboveMean = c(aboveMean, length(which(mat > mean(mat))))
    
    # Shannon entropy
    entropy = c(entropy, entropy(mat))
    
    # log10 (max value) - log10 (min value)
    #magOrder = c(magOrder, log10(max(mat))-log10(min(mat)))
  }
}
```

```{r}
# Attaching all of our newly-created features...
df = cbind(df, data.frame("norm"=norm))
df = cbind(df, data.frame("min"=min))
df = cbind(df, data.frame("max"=max))
df = cbind(df, data.frame("q1"=q1))
df = cbind(df, data.frame("q3"=q3))
df = cbind(df, data.frame("iqr"=iqr))
df = cbind(df, data.frame("mean"=mean))
df = cbind(df, data.frame("med"=med))
df = cbind(df, data.frame("std"=std))
df = cbind(df, data.frame("range"=range))
df = cbind(df, data.frame("skew"=skew))
df = cbind(df, data.frame("kurtosis"=kurtosis))
df = cbind(df, data.frame("aboveMeanCount"=aboveMean))
df = cbind(df, data.frame("entropy"=entropy))
#df = cbind(df, data.frame("magOrder"=magOrder))
```

Below is Doug's code to take the log of certain variables. 

```{r}
# Zeroes
zeroes_fix = df$zeroes
zeroes_fix = ifelse(zeroes_fix==0,zeroes_fix+.1,zeroes_fix)
zeros.log = log(zeroes_fix)
df = cbind(df, data.frame("zeros.log"=zeros.log))
#hist(zeros.log)
#skewness(df$zeros.log)

# Norm
norm_fix = df$norm
norm_fix = ifelse(norm_fix==0,norm_fix+.1,norm_fix)
norm.log = log(norm_fix)
df = cbind(df, data.frame("norm.log"=norm.log))
#hist(norm.log)
#print(skewness(norm.log))

# Min
min_fix = df$min
min_fix = ifelse(min_fix==0,min_fix+.1,min_fix)
min.log = log(min_fix)
df = cbind(df, data.frame("min.log"=min.log))
#hist(min.log)
#print(skewness(min.log))

# Max
max_fix = df$max
max_fix = ifelse(max_fix==0,max_fix+.1,max_fix)
max.log = log(max_fix)
df = cbind(df, data.frame("max.log"=max.log))
#hist(max.log)
#print(skewness(max.log))

# Q1
q1_fix = df$q1
q1_fix = ifelse(q1_fix==0,q1_fix+.1,q1_fix)
q1.log = log(q1_fix)
df = cbind(df, data.frame("q1.log"=q1.log))
#hist(q1.log)
#print(skewness(q1.log))

# Q3
q3_fix = df$q3
q3_fix = ifelse(q3_fix==0,q3_fix+.1,q3_fix)
q3.log = log(q3_fix)
df = cbind(df, data.frame("q3.log"=q3.log))
#hist(q3.log)
#print(skewness(q3.log))

# Med
med_fix = df$med
med_fix = ifelse(med_fix==0,med_fix+.1,med_fix)
med.log = log(med_fix)
df = cbind(df, data.frame("med.log"=med.log))
#hist(med.log)
#print(skewness(med.log))

# Std
std_fix = df$std
std_fix = ifelse(std_fix==0,std_fix+.1,std_fix)
std.log = log(std_fix)
df = cbind(df, data.frame("std.log"=std.log))
#hist(std.log)
#print(skewness(std.log))

# Range
range_fix = df$range
range_fix = ifelse(range_fix==0,range_fix+.1,range_fix)
range.log = log(range_fix)
df = cbind(df, data.frame("range.log"=range.log))
#hist(range.log)
#print(skewness(range.log))

# Skew
skew_fix = df$skew
skew_fix = ifelse(skew_fix==0,skew_fix+.1,skew_fix)
skew.log = log(skew_fix)
df = cbind(df, data.frame("skew.log"=skew.log))
#hist(skew.log)
#print(skewness(skew.log))

# Kurtosis
kurtosis_fix = df$kurtosis
kurtosis_fix = ifelse(kurtosis_fix==0,kurtosis_fix+.1,kurtosis_fix)
kurtosis.log = log(kurtosis_fix)
df = cbind(df, data.frame("kurtosis.log"=kurtosis.log))
#hist(kurtosis.log)
#print(skewness(kurtosis.log))

# AboveMeanCount
aboveMeanCount_fix = df$aboveMeanCount
aboveMeanCount_fix = ifelse(aboveMeanCount_fix==0,aboveMeanCount_fix+.1,aboveMeanCount_fix)
aboveMeanCount.log = log(aboveMeanCount_fix)
df = cbind(df, data.frame("aboveMeanCount.log"=aboveMeanCount.log))
#hist(aboveMeanCount.log)
#print(skewness(aboveMeanCount.log))
```

Adding in Jose's contour data. I saved this to a CSV that is in our /data folder. We will do a simple cbind() to attach the two variables.

```{r}
contour = read_csv("../data/contourData.csv")
```

```{r}
df = cbind(df,contour)
```

'df' is our (current) finalized dataframe with all our new features.

```{r}
# Change file path as needed
#write.csv(df, "/Users/margaretsabelhaus/Documents/GitHub/csci561-finalproject-team2/data/df.csv", row.names = FALSE)
```


Now we standardize our features:

```{r}
# Don't scale the target or dummy for whether it's in train/test set
standardized.df = data.frame(scale(df[,c("sum","norm","min","max","q1","q3",
                                         "iqr","mean","med","std","range","skew",
                                         "kurtosis")]))
standardized.df$classification = df$classification
standardized.df$validate = df$validate
standardized.df$zeroes = df$zeroes
standardized.df$aboveMeanCount = df$aboveMeanCount
standardized.df$entropy = df$entropy
standardized.df$zeros.log = df$zeros.log
standardized.df$norm.log = df$norm.log
standardized.df$min.log = df$min.log
standardized.df$max.log = df$max.log
standardized.df$q1.log = df$q1.log
standardized.df$q3.log = df$q3.log
standardized.df$med.log = df$med.log
standardized.df$std.log = df$std.log
standardized.df$range.log = df$range.log
standardized.df$skew.log = df$skew.log
standardized.df$kurtosis.log = df$kurtosis.log
standardized.df$aboveMeanCount.log = df$aboveMeanCount.log
standardized.df$contourCountLSD = df$contourCountLSD
standardized.df$contourAvgLength = df$contourAvgLength
standardized.df$contourLengthLSD = df$contourLengthLSD
standardized.df$contourAvgLengthLSD = df$contourAvgLengthLSD
```

```{r}
# Change file path as needed
#write.csv(standardized.df, "/Users/margaretsabelhaus/Documents/GitHub/csci561-finalproject-team2/data/standardizeddf.csv", row.names = FALSE)
```


We can play around with which features, if any, we want to scale. All you need to do is exclude those you don't want scaled in line 174, then add them to the standardized.df as per lines 175 and 176.

# Applying convolutions

Convolutions and max pooling in R: https://rpubs.com/eR_ic/conv_pools.

Here are a few things we need to know first about the images we are dealing with:

  - All images have dimensions 288 x 192
  - All images  have 1 depth of 1 (binary).
  - We visualize this 1 dimensional range of values on a red-blue color scale.
  - The range of values for each image differs by the type of data
  - Number of images per dataset are as follows:
    - low_train: 110
    - med_train: 240
    - high_train: 360
    - low_validate: 40
    - med_validate: 90
    - high_validate: 140
  - Therefore there are a total of 980 images (270 validation + 710 training)

In order to perform convolutions we will need to. . .

  1. Normalize the pixel values in each image by
   - Adjusting the range to have a 0 minimum
   - Divide by maximum value in each image

  2. Reshape the data into single arrays for training, testing, and validation.

  3. Two paths: (https://stackoverflow.com/questions/43485361/whats-the-difference-between-conv-layer-and-pooling-layer-in-cnn/43491146)
   - Pooling
   - Convolution + Pooling


1. Normalize the data using the max value in each of the images.
```{r}
#Normalize the data.

#Function that takes in a single image input (288x192 pixels) and rescales pixels to go 0 to max-min and normalizes them dividing by max-min.
normalize_image_pixels <- function(image){
  
  #Get the minumum value.
  min_val = min(image)

  #Subtract the lowest value from each value in the entire image.
  #This will make it so that the range of values is from 0 to (max - min).
  image = image - min_val

  #Get the new max value.
  max_val = max(image)

  #Divide the pixel values by the max value and return the result
  return(image / max_val)

}

#Normalize the data in the training and validation images. 
low_train_norm = low_train
for (i in 1:length(low_train_norm$mat)){
  low_train_norm$mat[[i]] = normalize_image_pixels(low_train_norm$mat[[i]])
}

med_train_norm = med_train
for (i in 1:length(med_train_norm$mat)){
  med_train_norm$mat[[i]] = normalize_image_pixels(med_train_norm$mat[[i]])
}

high_train_norm = high_train
for (i in 1:length(high_train_norm$mat)){
  high_train_norm$mat[[i]] = normalize_image_pixels(high_train_norm$mat[[i]])
}

low_validate_norm = low_validate
for (i in 1:length(low_validate_norm$mat)){
  low_validate_norm$mat[[i]] = normalize_image_pixels(low_validate_norm$mat[[i]])
}

med_validate_norm = med_validate
for (i in 1:length(med_validate_norm$mat)){
  med_validate_norm$mat[[i]] = normalize_image_pixels(med_validate_norm$mat[[i]])
}

high_validate_norm = high_validate
for (i in 1:length(high_validate_norm$mat)){
  high_validate_norm$mat[[i]] = normalize_image_pixels(high_validate_norm$mat[[i]])
}

```

2. Reshape the normalized images.

```{r}
#Get normalized dataset lengths
lt = length(low_train_norm$mat)
mt = length(med_train_norm$mat)
ht = length(high_train_norm$mat)
lv = length(low_validate_norm$mat)
mv = length(med_validate_norm$mat)
hv = length(high_validate_norm$mat)

#Get pixel values
training_images = c(low_train_norm$mat, med_train_norm$mat, high_train_norm$mat)
validation_images = c(low_validate_norm$mat, med_validate_norm$mat, high_validate_norm$mat)

#Reshape data into single array
training_images_rs = array_reshape(training_images, c(lt+mt+ht, 288, 192, 1))
validation_images_rs = array_reshape(validation_images, c(lv+mv+hv, 288, 192, 1))

```

3. Let's try pooling first. 

Using pooling layers alone treats our convolution filter as a matrix of ones. 
Therefore we are just taking a summary statistic of the values within the bounds of a filter.

The resulting output, when using the "valid" padding option, has a spatial shape (number of rows or columns) of: 
output_shape = math.floor((input_shape - pool_size) / strides) + 1 (when input_shape >= pool_size)

The resulting output shape when using the "same" padding option is: output_shape = math.floor((input_shape - 1) / strides) + 1

Pooling types:
  - Max pool: https://keras.rstudio.com/reference/layer_max_pooling_2d.html

  
```{r}
#Instantiate the convolution
small_pools <- keras_model_sequential()

  #specify input shape
  layer_input(shape=c(288, 192, 1))

  # adding a max pooling layer which halves the dimensions
  layer_max_pooling_2d(pool_size = c(2,2), strides=c(1,1), padding='valid')


```

4. Let's add convolution filters to the mix.

```{r}
#Instantiate the convolution
model <- keras_model_sequential()
  # adding the first convolution layer with 64 3by3 filter
  # we add a color depth of 1 since convolutions operate over 3D tensors
  layer_conv_2d(input_shape = c(288, 192, 1), filters = 6, kernel_size = c(3,3), activation = 'relu')
  # adding a max pooling layer which halves the dimensions
  layer_max_pooling_2d(pool_size = c(2,2))
  # adding a second convolution layer which filters the results
  # from the previous layer
  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = 'relu') # adding a pooling layer
  layer_max_pooling_2d(pool_size = c(2,2))

```