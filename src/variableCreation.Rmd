---
title: "561 Final Project"
author: "Team 2"
date: "11/23/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(Matrix)
library(dplyr)
#library(resample)
#library(ISLR)
#library(tree)
#library(gbm)
#library(MASS)
#library(class)
library(e1071) 
library(keras)
library(tensorflow)
library(entropy)

#Use this in command line if tensorflow.keras not found error
#use_condaenv("r-tensorflow")
```

Reading in data:

```{r}
# Set working directory as needed
setwd("/Users/margaretsabelhaus/Documents/GitHub/csci561-finalproject-team2")

load("./data/High/high_train.RData")
load("./data/Medium/med_train.RData")
load("./data/Low/low_train.RData")
load("./data/High/high_validate.RData")
load("./data/Medium/med_validate.RData")
load("./data/Low/low_validate.RData")
```

Try plotting:

```{r}
# Select data and axis labels
n=170
selected_dataset = med_train # copy of high_train list of datasets
l = length(selected_dataset$mat) # number of datasets in high_train
latitudes = round(as.numeric(colnames(selected_dataset$mat[[n]])))
longitudes = as.numeric(rownames(selected_dataset$mat[[n]]))
dataset = selected_dataset$mat[[n]] 
varname = selected_dataset$var[n]

# Plot
image(dataset, main=varname, col = hcl.colors(100, "Blue-Red"),
      axes=FALSE, xlab="Longitude", ylab="Latitude")
axis(3, at=seq(0,1, length=7), labels=longitudes[seq(1, 288, length.out=7)],
     lwd=0, pos=-0.2, outer=T)
axis(2, at=seq(1,0, length=9), labels=latitudes[seq(1, 192, length.out=9)],
     lwd=0, pos=0)
```

Let's use Dr. Hammerling's code to initialize our dataframe of [datasets, features]:

```{r}
df = data.frame()
ds_list = list(high_train, med_train, low_train, high_validate, med_validate, low_validate)
classifications = c("high", "med", "low")
class_index = rep(c(1, 2, 3), 2)
for (i in 1:length(ds_list)){
  l = length(ds_list[[i]]$mat)
  sum_feature = rep(NA, l)
  for(j in 1:l){
    sum_feature[j] = sum(ds_list[[i]]$mat[[j]])
  }
  classification = classifications[class_index[i]]
  
  # -- Margaret adding in; this is dummy for whether or not the 
  # dataset is in our validation set.
  test = ifelse(i %in% c(4,5,6),1,0)
  
  df = rbind(df, data.frame("sum"=sum_feature, "classification"=rep(classification, l), "validate"=rep(test,l)))
}
df$classification = as.factor(df$classification)

# Append a new feature, this one is the number of zeroes in each dataset, a measure of the sparsity of the dataset.
ds_list = list(high_train, med_train, low_train, high_validate, med_validate, low_validate)
z = c()
for (i in 1:length(ds_list)){
  l = length(ds_list[[i]]$mat)
  zero_feature = rep(NA, l)
  for(j in 1:l){
    zero_feature[j] = sum(ds_list[[i]]$mat[[j]] == 0)
  }
  z = c(z, zero_feature)
}
df = cbind(df, data.frame("zeroes"=z))
```

# Creating Summary Statistic Features

I'm going to loop through the datasets to create a bunch of new features. I'm sure there's a faster way to do this, but let's keep it simple for now 

```{r}
# Initialize empty vectors to populate with values for each dataset
norm = c()
min = c()
max = c()
q3 = c()
q1 = c()
iqr = c()
mean = c()
med = c()
std = c()
range = c()
skew = c()
kurtosis = c()
aboveMean = c()
entropy = c()
#magOrder = c()

# Loop through datasets and populate vectors
for (i in 1:length(ds_list)){
  numMats = length(ds_list[[i]]$mat)
  
  for (j in 1:numMats){
  
    mat = ds_list[[i]]$mat[[j]]
    
    # Norm
    norm = c(norm, norm(mat))
    
    # Min/Max
    min = c(min, min(mat))
    max = c(max, max(mat))
    
    # Quantiles, IQR
    q1 = c(q1, quantile(mat, .25))
    q3 = c(q3, quantile(mat, .75))
    iqr = c(iqr, quantile(mat, .25) - quantile(mat, .75))
    
    # Measures of center
    med = c(med, median(mat))
    mean = c(mean, mean(mat))
    
    # Measures of spread
    std = c(std, sd(mat))
    range = c(range, max(mat) - min(mat))
    
    # Skewness and Kurtosis
    skew = c(skew, skewness(mat))
    kurtosis = c(kurtosis, kurtosis(mat))
    
    # How many elements > mean
    aboveMean = c(aboveMean, length(which(mat > mean(mat))))
    
    # Shannon entropy
    entropy = c(entropy, entropy(mat))
    
    # log10 (max value) - log10 (min value)
    #magOrder = c(magOrder, log10(max(mat))-log10(min(mat)))
  }
}
```

```{r}
# Attaching all of our newly-created features...
df = cbind(df, data.frame("norm"=norm))
df = cbind(df, data.frame("min"=min))
df = cbind(df, data.frame("max"=max))
df = cbind(df, data.frame("q1"=q1))
df = cbind(df, data.frame("q3"=q3))
df = cbind(df, data.frame("iqr"=iqr))
df = cbind(df, data.frame("mean"=mean))
df = cbind(df, data.frame("med"=med))
df = cbind(df, data.frame("std"=std))
df = cbind(df, data.frame("range"=range))
df = cbind(df, data.frame("skew"=skew))
df = cbind(df, data.frame("kurtosis"=kurtosis))
df = cbind(df, data.frame("aboveMeanCount"=aboveMean))
df = cbind(df, data.frame("entropy"=entropy))
#df = cbind(df, data.frame("magOrder"=magOrder))
```

Below is Doug's code to take the log of certain variables. 

```{r}
# Zeroes
zeroes_fix = df$zeroes
zeroes_fix = ifelse(zeroes_fix==0,zeroes_fix+.1,zeroes_fix)
zeros.log = log(zeroes_fix)
df = cbind(df, data.frame("zeros.log"=zeros.log))
#hist(zeros.log)
#skewness(df$zeros.log)

# Norm
norm_fix = df$norm
norm_fix = ifelse(norm_fix==0,norm_fix+.1,norm_fix)
norm.log = log(norm_fix)
df = cbind(df, data.frame("norm.log"=norm.log))
#hist(norm.log)
#print(skewness(norm.log))

# Min
min_fix = df$min
min_fix = ifelse(min_fix==0,min_fix+.1,min_fix)
min.log = log(min_fix)
df = cbind(df, data.frame("min.log"=min.log))
#hist(min.log)
#print(skewness(min.log))

# Max
max_fix = df$max
max_fix = ifelse(max_fix==0,max_fix+.1,max_fix)
max.log = log(max_fix)
df = cbind(df, data.frame("max.log"=max.log))
#hist(max.log)
#print(skewness(max.log))

# Q1
q1_fix = df$q1
q1_fix = ifelse(q1_fix==0,q1_fix+.1,q1_fix)
q1.log = log(q1_fix)
df = cbind(df, data.frame("q1.log"=q1.log))
#hist(q1.log)
#print(skewness(q1.log))

# Q3
q3_fix = df$q3
q3_fix = ifelse(q3_fix==0,q3_fix+.1,q3_fix)
q3.log = log(q3_fix)
df = cbind(df, data.frame("q3.log"=q3.log))
#hist(q3.log)
#print(skewness(q3.log))

# Med
med_fix = df$med
med_fix = ifelse(med_fix==0,med_fix+.1,med_fix)
med.log = log(med_fix)
df = cbind(df, data.frame("med.log"=med.log))
#hist(med.log)
#print(skewness(med.log))

# Std
std_fix = df$std
std_fix = ifelse(std_fix==0,std_fix+.1,std_fix)
std.log = log(std_fix)
df = cbind(df, data.frame("std.log"=std.log))
#hist(std.log)
#print(skewness(std.log))

# Range
range_fix = df$range
range_fix = ifelse(range_fix==0,range_fix+.1,range_fix)
range.log = log(range_fix)
df = cbind(df, data.frame("range.log"=range.log))
#hist(range.log)
#print(skewness(range.log))

# Skew
skew_fix = df$skew
skew_fix = ifelse(skew_fix==0,skew_fix+.1,skew_fix)
skew.log = log(skew_fix)
df = cbind(df, data.frame("skew.log"=skew.log))
#hist(skew.log)
#print(skewness(skew.log))

# Kurtosis
kurtosis_fix = df$kurtosis
kurtosis_fix = ifelse(kurtosis_fix==0,kurtosis_fix+.1,kurtosis_fix)
kurtosis.log = log(kurtosis_fix)
df = cbind(df, data.frame("kurtosis.log"=kurtosis.log))
#hist(kurtosis.log)
#print(skewness(kurtosis.log))

# AboveMeanCount
aboveMeanCount_fix = df$aboveMeanCount
aboveMeanCount_fix = ifelse(aboveMeanCount_fix==0,aboveMeanCount_fix+.1,aboveMeanCount_fix)
aboveMeanCount.log = log(aboveMeanCount_fix)
df = cbind(df, data.frame("aboveMeanCount.log"=aboveMeanCount.log))
#hist(aboveMeanCount.log)
#print(skewness(aboveMeanCount.log))
```

Adding in Jose's contour data. I saved this to a CSV that is in our /data folder. We will do a simple cbind() to attach the two variables.

```{r}
contour = read_csv("../data/contourData.csv")
```

```{r}
df = cbind(df,contour)
```

'df' is our (current) finalized dataframe with all our new features.

Adding in Collin's pooling data.

```{r}
pooling = read_csv("pooling/train_validation_poolingFeatures.csv")
```
Cleaning it up a bit so we can attach to 'df'

```{r}
pooling = pooling[-c(1:4),-1]
```

Add to 'df'
```{r}
df = cbind(df,pooling)
```

```{r}
# Change file path as needed
#write.csv(df, "/Users/margaretsabelhaus/Documents/GitHub/csci561-finalproject-team2/data/df.csv", row.names = FALSE)
```


Now we standardize our features:

```{r}
# Don't scale the target or dummy for whether it's in train/test set
standardized.df = data.frame(scale(df[,c("sum","norm","min","max","q1","q3",
                                         "iqr","mean","med","std","range","skew",
                                         "kurtosis")]))
standardized.df$classification = df$classification
standardized.df$validate = df$validate
standardized.df$zeroes = df$zeroes
standardized.df$aboveMeanCount = df$aboveMeanCount
standardized.df$entropy = df$entropy
standardized.df$zeros.log = df$zeros.log
standardized.df$norm.log = df$norm.log
standardized.df$min.log = df$min.log
standardized.df$max.log = df$max.log
standardized.df$q1.log = df$q1.log
standardized.df$q3.log = df$q3.log
standardized.df$med.log = df$med.log
standardized.df$std.log = df$std.log
standardized.df$range.log = df$range.log
standardized.df$skew.log = df$skew.log
standardized.df$kurtosis.log = df$kurtosis.log
standardized.df$aboveMeanCount.log = df$aboveMeanCount.log
standardized.df$contourCountLSD = df$contourCountLSD
standardized.df$contourAvgLength = df$contourAvgLength
standardized.df$contourLengthLSD = df$contourLengthLSD
standardized.df$contourAvgLengthLSD = df$contourAvgLengthLSD
standardized.df$VarPool_75x25_MaxStat = df$VarPool_75x25_MaxStat
standardized.df$StdPool_75x25_VarStat = df$StdPool_75x25_VarStat
standardized.df$StdPool_75x25_StdStat = df$StdPool_75x25_StdStat
standardized.df$VarPool_75x25_StdStat = df$VarPool_75x25_StdStat
standardized.df$VarPool_75x25_MeanStat = df$VarPool_75x25_MeanStat
```

```{r}
# Change file path as needed
#write.csv(standardized.df, "/Users/margaretsabelhaus/Documents/GitHub/csci561-finalproject-team2/data/standardizeddf.csv", row.names = FALSE)
```


We can play around with which features, if any, we want to scale. All you need to do is exclude those you don't want scaled in line 174, then add them to the standardized.df as per lines 175 and 176.